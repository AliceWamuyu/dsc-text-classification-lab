{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification - Lab\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this lab, we'll use everything we've learned so far to build a model that can classify a text document as one of many possible classes!\n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "- Perform classification using a text dataset, using sensible preprocessing, tokenization, and feature engineering scheme \n",
    "- Use scikit-learn text vectorizers to fit and transform text data into a format to be used in a ML model \n",
    "\n",
    "\n",
    "\n",
    "# Getting Started\n",
    "\n",
    "For this lab, we'll be working with the classic **_Newsgroups Dataset_**, which is available as a training data set in `sklearn.datasets`. This dataset contains many different articles that fall into 1 of 20 possible classes. Our goal will be to build a classifier that can accurately predict the class of an article based on the features we create from the article itself!\n",
    "\n",
    "Let's get started. Run the cell below to import everything we'll need for this lab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk import word_tokenize, FreqDist\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to fetch our dataset. Run the cell below to download all the newsgroups articles and their corresponding labels. If this is the first time working with this dataset, scikit-learn will need to download all of the articles from an external repository -- the cell below may take a little while to run. \n",
    "\n",
    "The actual dataset is quite large. To save us from extremely long runtimes, we'll work with only a subset of the classes. Here is a list of all the possible classes:\n",
    "\n",
    "<img src='classes.png'>\n",
    "\n",
    "For this lab, we'll only work with the following five:\n",
    "\n",
    "* `'alt.atheism'`\n",
    "* `'comp.windows.x'`\n",
    "* `'rec.sport.hockey'`\n",
    "* `'sci.crypt'`\n",
    "* `'talk.politics.guns'`\n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Create a list called `categories` that contains the five newsgroups classes listed above, as strings \n",
    "* Get the training set by calling `fetch_20newsgroups()` and passing in the following parameters:\n",
    "    * `subset='train'`\n",
    "    * `categories=categories`\n",
    "    * `remove=('headers', 'footers', 'quotes')` -- this is so that the model can't overfit to metadata included in the articles that sometimes acts as a dead-giveaway as to what class the article belongs to  \n",
    "* Get the testing set as well by passing in the same parameters, with the exception of `subset='test` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['alt.atheism', 'comp.windows.x', 'rec.sport.hockey', 'sci.crypt', 'talk.politics.guns']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories, remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Let's break apart the data and the labels, and then inspect the class names to see what the actual newsgroups are.\n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Grab the data from `newsgroups_train.data` and store it in the appropriate variable  \n",
    "* Grab the labels from `newsgroups_train.target` and store it in the appropriate variable  \n",
    "* Grab the label names from `newsgroups_train.target_names` and store it in the appropriate variable  \n",
    "* Display the `label_names` so that we can see the different classes of articles that we're working with, and confirm that we grabbed the right ones  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.windows.x',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'talk.politics.guns']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = newsgroups_train.data\n",
    "target = newsgroups_train.target\n",
    "label_names = newsgroups_train.target_names\n",
    "label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2814,)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'info'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [44]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtarget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'info'"
     ]
    }
   ],
   "source": [
    "target.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Note: These trial updates are summarized from reports in the\\n_Idaho Statesman_ and the local NBC affiliate television\\nstation, KTVB Channel 7.\\n\\nRandy Weaver/Kevin Harris trial update: Day 4.\\n\\nFriday, April 16, 1993 was the fourth day of the trial.\\n\\nSynopsis: Defense attorney Gerry Spence cross-examined agent\\nCooper under repeated objections from prosecutor Ronald\\nHowen.  Spence moved for a mistrial but was denied.\\n\\nThe day was marked by a caustic cross-examination of Deputy\\nMarshal Larry Cooper by defense attorney Gerry Spence.  Although\\nSpence has not explicitly stated so, one angle of his stategy\\nmust involve destroying the credibility of agent Cooper.  Cooper is\\nthe government\\'s only eyewitness to the death of agent Degan.\\nSpence attacked Cooper\\'s credibility by pointing out discrepancies\\nbetween Cooper\\'s statements last September and those made in court.\\nCooper conceded that, \"You have all these things compressed into\\na few seconds...It\\'s difficult to remember what went on first.\"\\n\\nCooper acknowledged that he carried a \"9mm Colt Commando submachine\\ngun with a silenced barrel.\" [I thought a Colt Commando was a revolver!]\\nCooper continued by stating that the federal agents had no specific\\nplans to use the weapon when they started to kill Weaver\\'s dog.\\n\\nWhen Spence asked how seven cartridges could be fired by Degan\\'s\\nM-16 rifle when Degan was apparently dead, Cooper could not say for\\nsure that Degan did not return fire before going down.\\n\\nSpence continued by asking with how many agents (and to what extent)\\nhad Cooper discussed last August\\'s events, Cooper responded, \"If\\nyou\\'re implying that we got our story together, you\\'re wrong,\\ncounselor.\"  Spence continued to advance the defense\\'s version of\\nthe events: Namely, that a marshal had started the shooting by\\nkilling the Weaver\\'s dog.  Cooper disagreed.\\n\\nAssistant U.S. Attorney Ronald Howen repeatedly objected to Spence\\'s\\nvirulent cross-examination of agent Cooper, arguing that the questions\\nwere repetitive and Spence was wasting time.  Howen also complained \\nthat Spence was improperly using a cross-examination to advance the\\ndefense\\'s version of the events.  U.S. District Judge Edward Lodge\\nsustained many of the objections; however, both lawyers persisted\\nuntil Judge Lodge had the jury leave the room and proceded to\\nadmonish both attorneys.  \"I\\'m not going to play games with either\\ncounsel.  This has been a personality problem from day 1, so start\\nacting like professionals.\"\\n\\nSpence told the judge that, \"When all the evidence is in, we\\'ll see\\nthat ... his [agent Larry Cooper] testimony is not credible, that\\nhe was panicked and cannot remember the sequence of events.\" \\nSpence continued, \"We\\'re going to find...that there is a very unlikely\\nsimilarity - almost as if it had come out of a cookie cutter - between\\nthe testimony of Mr. Cooper and the other witnesses.\"\\n\\nSpence then moved for a mistrial on the grounds that Howen\\'s repeated\\nobjections would prevent a fair trial, \"We can\\'t have a fair trial if the\\njury believes I\\'m some sort of charlatan, if the jury believes I\\'m\\nbending the rules or engaging in some delaying tactic or that I\\'m\\nviolating court orders.\"\\n\\nJudge Lodge called the notion that his repeated sustainings of Howen\\'s\\nobjections had somehow prejudiced the jury was \"preposterous\" and\\ndenied the motion for a mistrial.  Lodge did tell Howen to restrict\\nhis comments when objecting.\\n\\nThe trial resumed with the prosecution calling FBI Special Agent Greg\\nRampton.  The prosecution\\'s purpose was simply to introduce five\\nweapons found in the cabin as evidence: However, the defense seized\\non the opportunity to further address Cooper\\'s credibility.\\n\\nDefense attorney Ellison Matthews (Harris\\' other attorney) questioned\\nRampton about the dog.  Rampton stated that there were no specific\\nplans to kill the Weaver\\'s dog without being detected.  Matthews then\\nhad Rampton read  a Septtember 15, 1992 transcript in which Rampton\\nhad said that Cooper had said that the purpose of the silenced weapon\\nwas to kill the dog without being detected, if the dog chased them.\\nRampton then acknowledged that he believed that Cooper had said that,\\nbut he could not remember when.  He then stated that, \"I did not conduct\\nthe primary interview with Deputy Cooper, but I have had conversations\\nwith him since the interview was conducted.\"'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's check the shape of `data` to see what our data looks like. We can do this by checking the `.shape` attribute of `newsgroups_train.filenames`.\n",
    "\n",
    "Do this now in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2814,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here\n",
    "newsgroups_train.filenames.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset contains 2,814 different articles spread across the five classes we chose. \n",
    "\n",
    "### Cleaning and Preprocessing Our Data\n",
    "\n",
    "Now that we have our data, the fun part begins. We'll need to begin by preprocessing and cleaning our text data. As you've seen throughout this section, preprocessing text data is a bit more challenging that working with more traditional data types because there's no clear-cut answer for exactly what sort of preprocessing and cleaning we need to do. Before we can begin cleaning and preprocessing our text data, we need to make some decisions about things such as:\n",
    "\n",
    "* Do we remove stop words or not?\n",
    "* Do we stem or lemmatize our text data, or leave the words as is?\n",
    "* Is basic tokenization enough, or do we need to support special edge cases through the use of regex?\n",
    "* Do we use the entire vocabulary, or just limit the model to a subset of the most frequently used words? If so, how many?\n",
    "* Do we engineer other features, such as bigrams, or POS tags, or Mutual Information Scores?\n",
    "* What sort of vectorization should we use in our model? Boolean Vectorization? Count Vectorization? TF-IDF? More advanced vectorization strategies such as Word2Vec?\n",
    "\n",
    "\n",
    "These are all questions that we'll need to think about pretty much anytime we begin working with text data. \n",
    "\n",
    "Let's get right into it. We'll start by getting a list of all of the english stopwords, and concatenating them with a list of all the punctuation. \n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Get all the english stopwords from `nltk` \n",
    "* Get all of the punctuation from `string.punctuation`, and convert it to a list \n",
    "* Add the two lists together. Name the result `stopwords_list` \n",
    "* Create another list containing various types of empty strings and ellipses, such as `[\"''\", '\"\"', '...', '``']`. Add this to our `stopwords_list`, so that we won't have tokens that are only empty quotes and such  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopw = stopwords.words('english')\n",
    "punct = list(string.punctuation)\n",
    "stopwords_list = stopw + punct\n",
    "stopwords_list += [\"''\", '\"\"', '...', '``']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We'll leave these alone for now, until we're ready to remove stop words after the tokenization step. \n",
    "\n",
    "Next, let's try tokenizing our dataset. In order to save ourselves some time, we'll write a function to clean our dataset, and then use Python's built-in `map()` function to clean every article in the dataset at the same time. \n",
    "\n",
    "In the cell below, complete the `process_article()` function. This function should:\n",
    "\n",
    "* Take in one parameter, `article` \n",
    "* Tokenize the article using the appropriate function from `nltk` \n",
    "* Lowercase every token, remove any stopwords found in `stopwords_list` from the tokenized article, and return the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_article(article):\n",
    "    tokens =nltk.word_tokenize(article)\n",
    "    filtered = []\n",
    "    for token in tokens:\n",
    "        lowercase_token = token.lower()\n",
    "    \n",
    "        if lowercase_token not in stopwords_list:\n",
    "            filtered.append(lowercase_token)\n",
    "    return filtered\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have this function, let's go ahead and preprocess our data, and then move into exploring our dataset. \n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Use Python's `map()` function and pass in two parameters: the `process_article` function and the `data`. Make sure to wrap the whole map statement in a `list()`.\n",
    "\n",
    "**_Note:_** Running this cell may take a minute or two!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = list(map(process_article, data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. Now, let's inspect the first article in `processed_data` to see how it looks. \n",
    "\n",
    "Do this now in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['note',\n",
       " 'trial',\n",
       " 'updates',\n",
       " 'summarized',\n",
       " 'reports',\n",
       " '_idaho',\n",
       " 'statesman_',\n",
       " 'local',\n",
       " 'nbc',\n",
       " 'affiliate',\n",
       " 'television',\n",
       " 'station',\n",
       " 'ktvb',\n",
       " 'channel',\n",
       " '7',\n",
       " 'randy',\n",
       " 'weaver/kevin',\n",
       " 'harris',\n",
       " 'trial',\n",
       " 'update',\n",
       " 'day',\n",
       " '4',\n",
       " 'friday',\n",
       " 'april',\n",
       " '16',\n",
       " '1993',\n",
       " 'fourth',\n",
       " 'day',\n",
       " 'trial',\n",
       " 'synopsis',\n",
       " 'defense',\n",
       " 'attorney',\n",
       " 'gerry',\n",
       " 'spence',\n",
       " 'cross-examined',\n",
       " 'agent',\n",
       " 'cooper',\n",
       " 'repeated',\n",
       " 'objections',\n",
       " 'prosecutor',\n",
       " 'ronald',\n",
       " 'howen',\n",
       " 'spence',\n",
       " 'moved',\n",
       " 'mistrial',\n",
       " 'denied',\n",
       " 'day',\n",
       " 'marked',\n",
       " 'caustic',\n",
       " 'cross-examination',\n",
       " 'deputy',\n",
       " 'marshal',\n",
       " 'larry',\n",
       " 'cooper',\n",
       " 'defense',\n",
       " 'attorney',\n",
       " 'gerry',\n",
       " 'spence',\n",
       " 'although',\n",
       " 'spence',\n",
       " 'explicitly',\n",
       " 'stated',\n",
       " 'one',\n",
       " 'angle',\n",
       " 'stategy',\n",
       " 'must',\n",
       " 'involve',\n",
       " 'destroying',\n",
       " 'credibility',\n",
       " 'agent',\n",
       " 'cooper',\n",
       " 'cooper',\n",
       " 'government',\n",
       " \"'s\",\n",
       " 'eyewitness',\n",
       " 'death',\n",
       " 'agent',\n",
       " 'degan',\n",
       " 'spence',\n",
       " 'attacked',\n",
       " 'cooper',\n",
       " \"'s\",\n",
       " 'credibility',\n",
       " 'pointing',\n",
       " 'discrepancies',\n",
       " 'cooper',\n",
       " \"'s\",\n",
       " 'statements',\n",
       " 'last',\n",
       " 'september',\n",
       " 'made',\n",
       " 'court',\n",
       " 'cooper',\n",
       " 'conceded',\n",
       " 'things',\n",
       " 'compressed',\n",
       " 'seconds',\n",
       " \"'s\",\n",
       " 'difficult',\n",
       " 'remember',\n",
       " 'went',\n",
       " 'first',\n",
       " 'cooper',\n",
       " 'acknowledged',\n",
       " 'carried',\n",
       " '9mm',\n",
       " 'colt',\n",
       " 'commando',\n",
       " 'submachine',\n",
       " 'gun',\n",
       " 'silenced',\n",
       " 'barrel',\n",
       " 'thought',\n",
       " 'colt',\n",
       " 'commando',\n",
       " 'revolver',\n",
       " 'cooper',\n",
       " 'continued',\n",
       " 'stating',\n",
       " 'federal',\n",
       " 'agents',\n",
       " 'specific',\n",
       " 'plans',\n",
       " 'use',\n",
       " 'weapon',\n",
       " 'started',\n",
       " 'kill',\n",
       " 'weaver',\n",
       " \"'s\",\n",
       " 'dog',\n",
       " 'spence',\n",
       " 'asked',\n",
       " 'seven',\n",
       " 'cartridges',\n",
       " 'could',\n",
       " 'fired',\n",
       " \"degan's\",\n",
       " 'm-16',\n",
       " 'rifle',\n",
       " 'degan',\n",
       " 'apparently',\n",
       " 'dead',\n",
       " 'cooper',\n",
       " 'could',\n",
       " 'say',\n",
       " 'sure',\n",
       " 'degan',\n",
       " 'return',\n",
       " 'fire',\n",
       " 'going',\n",
       " 'spence',\n",
       " 'continued',\n",
       " 'asking',\n",
       " 'many',\n",
       " 'agents',\n",
       " 'extent',\n",
       " 'cooper',\n",
       " 'discussed',\n",
       " 'last',\n",
       " 'august',\n",
       " \"'s\",\n",
       " 'events',\n",
       " 'cooper',\n",
       " 'responded',\n",
       " \"'re\",\n",
       " 'implying',\n",
       " 'got',\n",
       " 'story',\n",
       " 'together',\n",
       " \"'re\",\n",
       " 'wrong',\n",
       " 'counselor',\n",
       " 'spence',\n",
       " 'continued',\n",
       " 'advance',\n",
       " 'defense',\n",
       " \"'s\",\n",
       " 'version',\n",
       " 'events',\n",
       " 'namely',\n",
       " 'marshal',\n",
       " 'started',\n",
       " 'shooting',\n",
       " 'killing',\n",
       " 'weaver',\n",
       " \"'s\",\n",
       " 'dog',\n",
       " 'cooper',\n",
       " 'disagreed',\n",
       " 'assistant',\n",
       " 'u.s.',\n",
       " 'attorney',\n",
       " 'ronald',\n",
       " 'howen',\n",
       " 'repeatedly',\n",
       " 'objected',\n",
       " \"spence's\",\n",
       " 'virulent',\n",
       " 'cross-examination',\n",
       " 'agent',\n",
       " 'cooper',\n",
       " 'arguing',\n",
       " 'questions',\n",
       " 'repetitive',\n",
       " 'spence',\n",
       " 'wasting',\n",
       " 'time',\n",
       " 'howen',\n",
       " 'also',\n",
       " 'complained',\n",
       " 'spence',\n",
       " 'improperly',\n",
       " 'using',\n",
       " 'cross-examination',\n",
       " 'advance',\n",
       " 'defense',\n",
       " \"'s\",\n",
       " 'version',\n",
       " 'events',\n",
       " 'u.s.',\n",
       " 'district',\n",
       " 'judge',\n",
       " 'edward',\n",
       " 'lodge',\n",
       " 'sustained',\n",
       " 'many',\n",
       " 'objections',\n",
       " 'however',\n",
       " 'lawyers',\n",
       " 'persisted',\n",
       " 'judge',\n",
       " 'lodge',\n",
       " 'jury',\n",
       " 'leave',\n",
       " 'room',\n",
       " 'proceded',\n",
       " 'admonish',\n",
       " 'attorneys',\n",
       " \"'m\",\n",
       " 'going',\n",
       " 'play',\n",
       " 'games',\n",
       " 'either',\n",
       " 'counsel',\n",
       " 'personality',\n",
       " 'problem',\n",
       " 'day',\n",
       " '1',\n",
       " 'start',\n",
       " 'acting',\n",
       " 'like',\n",
       " 'professionals',\n",
       " 'spence',\n",
       " 'told',\n",
       " 'judge',\n",
       " 'evidence',\n",
       " \"'ll\",\n",
       " 'see',\n",
       " 'agent',\n",
       " 'larry',\n",
       " 'cooper',\n",
       " 'testimony',\n",
       " 'credible',\n",
       " 'panicked',\n",
       " 'remember',\n",
       " 'sequence',\n",
       " 'events',\n",
       " 'spence',\n",
       " 'continued',\n",
       " \"'re\",\n",
       " 'going',\n",
       " 'find',\n",
       " 'unlikely',\n",
       " 'similarity',\n",
       " 'almost',\n",
       " 'come',\n",
       " 'cookie',\n",
       " 'cutter',\n",
       " 'testimony',\n",
       " 'mr.',\n",
       " 'cooper',\n",
       " 'witnesses',\n",
       " 'spence',\n",
       " 'moved',\n",
       " 'mistrial',\n",
       " 'grounds',\n",
       " 'howen',\n",
       " \"'s\",\n",
       " 'repeated',\n",
       " 'objections',\n",
       " 'would',\n",
       " 'prevent',\n",
       " 'fair',\n",
       " 'trial',\n",
       " 'ca',\n",
       " \"n't\",\n",
       " 'fair',\n",
       " 'trial',\n",
       " 'jury',\n",
       " 'believes',\n",
       " \"'m\",\n",
       " 'sort',\n",
       " 'charlatan',\n",
       " 'jury',\n",
       " 'believes',\n",
       " \"i'm\",\n",
       " 'bending',\n",
       " 'rules',\n",
       " 'engaging',\n",
       " 'delaying',\n",
       " 'tactic',\n",
       " \"i'm\",\n",
       " 'violating',\n",
       " 'court',\n",
       " 'orders',\n",
       " 'judge',\n",
       " 'lodge',\n",
       " 'called',\n",
       " 'notion',\n",
       " 'repeated',\n",
       " 'sustainings',\n",
       " \"howen's\",\n",
       " 'objections',\n",
       " 'somehow',\n",
       " 'prejudiced',\n",
       " 'jury',\n",
       " 'preposterous',\n",
       " 'denied',\n",
       " 'motion',\n",
       " 'mistrial',\n",
       " 'lodge',\n",
       " 'tell',\n",
       " 'howen',\n",
       " 'restrict',\n",
       " 'comments',\n",
       " 'objecting',\n",
       " 'trial',\n",
       " 'resumed',\n",
       " 'prosecution',\n",
       " 'calling',\n",
       " 'fbi',\n",
       " 'special',\n",
       " 'agent',\n",
       " 'greg',\n",
       " 'rampton',\n",
       " 'prosecution',\n",
       " \"'s\",\n",
       " 'purpose',\n",
       " 'simply',\n",
       " 'introduce',\n",
       " 'five',\n",
       " 'weapons',\n",
       " 'found',\n",
       " 'cabin',\n",
       " 'evidence',\n",
       " 'however',\n",
       " 'defense',\n",
       " 'seized',\n",
       " 'opportunity',\n",
       " 'address',\n",
       " 'cooper',\n",
       " \"'s\",\n",
       " 'credibility',\n",
       " 'defense',\n",
       " 'attorney',\n",
       " 'ellison',\n",
       " 'matthews',\n",
       " 'harris',\n",
       " 'attorney',\n",
       " 'questioned',\n",
       " 'rampton',\n",
       " 'dog',\n",
       " 'rampton',\n",
       " 'stated',\n",
       " 'specific',\n",
       " 'plans',\n",
       " 'kill',\n",
       " 'weaver',\n",
       " \"'s\",\n",
       " 'dog',\n",
       " 'without',\n",
       " 'detected',\n",
       " 'matthews',\n",
       " 'rampton',\n",
       " 'read',\n",
       " 'septtember',\n",
       " '15',\n",
       " '1992',\n",
       " 'transcript',\n",
       " 'rampton',\n",
       " 'said',\n",
       " 'cooper',\n",
       " 'said',\n",
       " 'purpose',\n",
       " 'silenced',\n",
       " 'weapon',\n",
       " 'kill',\n",
       " 'dog',\n",
       " 'without',\n",
       " 'detected',\n",
       " 'dog',\n",
       " 'chased',\n",
       " 'rampton',\n",
       " 'acknowledged',\n",
       " 'believed',\n",
       " 'cooper',\n",
       " 'said',\n",
       " 'could',\n",
       " 'remember',\n",
       " 'stated',\n",
       " 'conduct',\n",
       " 'primary',\n",
       " 'interview',\n",
       " 'deputy',\n",
       " 'cooper',\n",
       " 'conversations',\n",
       " 'since',\n",
       " 'interview',\n",
       " 'conducted']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's move onto exploring the dataset a bit more. Let's start by getting the total vocabulary size of the training dataset. We can do this by creating a `set` object and then using it's `.update()` method to iteratively add each article. Since it's a set, it will only contain unique words, with no duplicates. \n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Create a `set()` object called `total_vocab` \n",
    "* Iterate through each tokenized article in `processed_data` and add it to the set using the set's `.update()` method \n",
    "* Once all articles have been added, get the total number of unique words in our training set by taking the length of the set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46332"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_vocab = set()\n",
    "for tokens in processed_data:\n",
    "    total_vocab.update(tokens)\n",
    "len(total_vocab) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Data With Frequency Distributions\n",
    "\n",
    "Great -- our processed dataset contains 46,990 unique words! \n",
    "\n",
    "Next, let's create a frequency distribution to see which words are used the most! \n",
    "\n",
    "In order to do this, we'll need to concatenate every article into a single list, and then pass this list to `FreqDist()`. \n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Create an empty list called `articles_concat` \n",
    "* Iterate through `processed_data` and add every article it contains to `articles_concat` \n",
    "* Pass `articles_concat` as input to `FreqDist()`  \n",
    "* Display the top 200 most used words  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_concat = []\n",
    "for articles in processed_data:\n",
    "    articles_concat += articles\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('--', 29501),\n",
       " ('x', 4851),\n",
       " (\"'s\", 3204),\n",
       " (\"n't\", 2938),\n",
       " ('1', 2541),\n",
       " ('would', 1986),\n",
       " ('0', 1984),\n",
       " ('one', 1763),\n",
       " ('2', 1676),\n",
       " ('people', 1245),\n",
       " ('use', 1147),\n",
       " ('get', 1068),\n",
       " ('like', 1037),\n",
       " ('file', 1024),\n",
       " ('3', 1011),\n",
       " ('also', 877),\n",
       " ('4', 875),\n",
       " ('key', 869),\n",
       " ('could', 856),\n",
       " ('know', 819),\n",
       " ('think', 817),\n",
       " ('time', 782),\n",
       " ('may', 730),\n",
       " ('even', 711),\n",
       " ('new', 706),\n",
       " ('5', 684),\n",
       " ('first', 678),\n",
       " ('system', 673),\n",
       " ('well', 670),\n",
       " ('information', 646),\n",
       " ('make', 644),\n",
       " ('right', 638),\n",
       " ('many', 636),\n",
       " ('see', 636),\n",
       " ('two', 634),\n",
       " ('good', 610),\n",
       " ('7', 603),\n",
       " ('used', 600),\n",
       " ('government', 589),\n",
       " ('way', 573),\n",
       " ('available', 568),\n",
       " ('window', 568),\n",
       " (\"'m\", 563),\n",
       " ('6', 553),\n",
       " ('db', 553),\n",
       " ('much', 546),\n",
       " ('encryption', 537),\n",
       " ('using', 527),\n",
       " ('say', 524),\n",
       " ('gun', 520),\n",
       " ('number', 518),\n",
       " ('program', 517),\n",
       " ('us', 510),\n",
       " ('team', 498),\n",
       " ('must', 486),\n",
       " ('law', 476),\n",
       " ('since', 449),\n",
       " ('need', 446),\n",
       " ('game', 440),\n",
       " ('chip', 439),\n",
       " ('something', 436),\n",
       " ('8', 436),\n",
       " ('want', 421),\n",
       " ('god', 420),\n",
       " ('server', 417),\n",
       " (\"'ve\", 416),\n",
       " ('public', 408),\n",
       " ('year', 401),\n",
       " ('set', 396),\n",
       " ('ca', 393),\n",
       " ('find', 392),\n",
       " ('please', 386),\n",
       " ('c', 386),\n",
       " ('point', 385),\n",
       " ('without', 383),\n",
       " ('might', 383),\n",
       " ('really', 381),\n",
       " ('read', 379),\n",
       " ('believe', 379),\n",
       " ('said', 378),\n",
       " ('go', 377),\n",
       " ('take', 377),\n",
       " ('version', 374),\n",
       " ('anyone', 372),\n",
       " ('second', 370),\n",
       " ('n', 368),\n",
       " ('list', 367),\n",
       " ('code', 367),\n",
       " ('another', 364),\n",
       " ('keys', 362),\n",
       " (\"'re\", 361),\n",
       " ('work', 361),\n",
       " ('example', 359),\n",
       " ('clipper', 358),\n",
       " ('play', 357),\n",
       " ('problem', 356),\n",
       " ('things', 353),\n",
       " ('data', 353),\n",
       " ('made', 348),\n",
       " ('widget', 345),\n",
       " ('still', 345),\n",
       " ('output', 345),\n",
       " ('sure', 344),\n",
       " ('however', 344),\n",
       " ('10', 343),\n",
       " ('case', 343),\n",
       " ('hockey', 341),\n",
       " ('back', 341),\n",
       " ('entry', 341),\n",
       " ('last', 340),\n",
       " (\"'d\", 336),\n",
       " ('let', 334),\n",
       " ('25', 334),\n",
       " ('better', 333),\n",
       " ('part', 330),\n",
       " ('security', 327),\n",
       " ('probably', 324),\n",
       " ('privacy', 323),\n",
       " ('subject', 322),\n",
       " ('line', 321),\n",
       " ('question', 320),\n",
       " ('going', 319),\n",
       " ('period', 315),\n",
       " ('name', 314),\n",
       " ('state', 312),\n",
       " ('course', 311),\n",
       " ('9', 310),\n",
       " ('anonymous', 307),\n",
       " ('years', 304),\n",
       " ('look', 301),\n",
       " ('true', 300),\n",
       " ('files', 300),\n",
       " ('got', 299),\n",
       " ('control', 298),\n",
       " ('long', 295),\n",
       " ('fact', 294),\n",
       " ('every', 293),\n",
       " ('application', 291),\n",
       " ('season', 290),\n",
       " (\"'ll\", 289),\n",
       " ('someone', 286),\n",
       " ('source', 284),\n",
       " ('possible', 283),\n",
       " ('help', 282),\n",
       " ('message', 280),\n",
       " ('never', 280),\n",
       " ('55.0', 279),\n",
       " ('games', 276),\n",
       " ('thing', 276),\n",
       " ('following', 276),\n",
       " ('send', 273),\n",
       " ('try', 272),\n",
       " ('best', 271),\n",
       " ('email', 270),\n",
       " ('motif', 270),\n",
       " ('general', 269),\n",
       " ('run', 269),\n",
       " ('rather', 268),\n",
       " ('actually', 265),\n",
       " ('means', 265),\n",
       " ('several', 264),\n",
       " ('thanks', 264),\n",
       " ('note', 263),\n",
       " ('either', 263),\n",
       " ('give', 263),\n",
       " ('keep', 262),\n",
       " ('little', 262),\n",
       " ('put', 262),\n",
       " ('different', 262),\n",
       " ('guns', 260),\n",
       " ('enough', 259),\n",
       " ('far', 256),\n",
       " ('given', 256),\n",
       " ('come', 255),\n",
       " ('group', 254),\n",
       " ('p', 253),\n",
       " ('seems', 252),\n",
       " ('..', 251),\n",
       " ('e', 251),\n",
       " ('around', 250),\n",
       " ('person', 249),\n",
       " ('order', 249),\n",
       " ('next', 248),\n",
       " ('call', 248),\n",
       " ('least', 247),\n",
       " ('support', 246),\n",
       " ('anything', 246),\n",
       " ('etc', 242),\n",
       " ('section', 240),\n",
       " ('internet', 238),\n",
       " ('power', 237),\n",
       " ('open', 236),\n",
       " ('sun', 236),\n",
       " ('great', 234),\n",
       " ('world', 233),\n",
       " ('user', 232),\n",
       " ('mail', 231),\n",
       " ('12', 230),\n",
       " ('real', 230),\n",
       " ('g', 230)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_freqdist = FreqDist(articles_concat)\n",
    "articles_freqdist.most_common(200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance, none of these words seem very informative -- for most of the words represented here, it would be tough to guess if a given word is used equally among all five classes, or is disproportionately represented among a single class. This makes sense, because this frequency distribution represents all the classes combined. This tells us that these words are probably the least important, as they are most likely words that are used across multiple classes, thereby providing our model with little actual signal as to what class they belong to. This tells us that we probably want to focus on words that appear heavily in articles from a given class, but rarely appear in articles from other classes. You may recall from previous lessons that this is exactly where **_TF-IDF Vectorization_** really shines!\n",
    "\n",
    "### Vectorizing with TF-IDF\n",
    "\n",
    "Although NLTK does provide functionality for vectorizing text documents with TF-IDF, we'll make use of scikit-learn's TF-IDF vectorizer, because we already have experience with it, and because it's a bit easier to use, especially when the models we'll be feeding the vectorized data into are from scikit-learn, meaning that we don't have to worry about doing any extra processing to ensure they play nicely together. \n",
    "\n",
    "Recall that in order to use scikit-learn's `TfidfVectorizer()`, we need to pass in the data as raw text documents -- the `TfidfVectorizer()` handles the count vectorization process on it's own, and then fits and transforms the data into TF-IDF format. \n",
    "\n",
    "This means that we need to:\n",
    "\n",
    "* Import `TfidfVectorizer` from `sklearn.feature_extraction.text` and instantiate `TfidfVectorizer()` \n",
    "* Call the vectorizer object's `.fit_transform()` method and pass in our `data` as input. Store the results in `tf_idf_data_train` \n",
    "* Also create a vectorized version of our testing data, which can be found in `newsgroups_test.data`. Store the results in `tf_idf_data_test`. \n",
    "\n",
    "\n",
    "**_NOTE:_** When transforming the test data, use the `.transform()` method, not the `.fit_transform()` method, as the vectorizer has already been fit to the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_data_train = vectorizer.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_data_test = vectorizer.transform(newsgroups_test.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling Our Data\n",
    "\n",
    "Great! We've now preprocessed and explored our dataset, let's take a second to see what our data looks like in vectorized form. \n",
    "\n",
    "In the cell below, get the shape of `tf_idf_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2814, 36622)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here\n",
    "tf_idf_data_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our vectorized data contains 2,814 articles, with 36,622 unique words in the vocabulary. However, the vast majority of these columns for any given article will be zero, since every article only contains a small subset of the total vocabulary. Recall that vectors mostly filled with zeros are referred to as **_Sparse Vectors_**. These are extremely common when working with text data. \n",
    "\n",
    "Let's check out the average number of non-zero columns in the vectors. Run the cell below to calculate this average. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Number of Non-Zero Elements in Vectorized Articles: 107.28038379530916\n",
      "Percentage of columns containing 0: 0.9970706028126451\n"
     ]
    }
   ],
   "source": [
    "non_zero_cols = tf_idf_data_train.nnz / float(tf_idf_data_train.shape[0])\n",
    "print(\"Average Number of Non-Zero Elements in Vectorized Articles: {}\".format(non_zero_cols))\n",
    "\n",
    "percent_sparse = 1 - (non_zero_cols / float(tf_idf_data_train.shape[1]))\n",
    "print('Percentage of columns containing 0: {}'.format(percent_sparse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the output above, the average vectorized article contains 107 non-zero columns. This means that 99.7% of each vector is actually zeroes! This is one reason why it's best not to create your own vectorizers, and rely on professional packages such as scikit-learn and NLTK instead -- they contain many speed and memory optimizations specifically for dealing with sparse vectors. This way, we aren't wasting a giant chunk of memory on a vectorized dataset that only has valid information in 0.3% of it. \n",
    "\n",
    "Now that we've vectorized our dataset, let's create some models and fit them to our vectorized training data. \n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Instantiate `MultinomialNB()` and `RandomForestClassifier()`. For random forest, set `n_estimators` to `100`. Don't worry about tweaking any of the other parameters  \n",
    "* Fit each to our vectorized training data \n",
    "* Create predictions for our training and test sets\n",
    "* Calculate the `accuracy_score()` for both the training and test sets (you'll find our training labels stored within the variable `target`, and the test labels stored within `newsgroups_test.target`) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classifier = MultinomialNB()\n",
    "rf_classifier = RandomForestClassifier(n_estimators = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 27337 features, but MultinomialNB is expecting 36622 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [45]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m nb_classifier\u001b[38;5;241m.\u001b[39mfit(tf_idf_data_train, target)\n\u001b[0;32m      2\u001b[0m nb_train_preds \u001b[38;5;241m=\u001b[39m nb_classifier\u001b[38;5;241m.\u001b[39mpredict(tf_idf_data_train)\n\u001b[1;32m----> 3\u001b[0m nb_test_preds \u001b[38;5;241m=\u001b[39m \u001b[43mnb_classifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf_idf_data_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Users\\ThisPC\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:82\u001b[0m, in \u001b[0;36m_BaseNB.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;124;03mPerform classification on an array of test vectors X.\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;124;03m    Predicted target values for X.\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     81\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m---> 82\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_X\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m jll \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_joint_log_likelihood(X)\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_[np\u001b[38;5;241m.\u001b[39margmax(jll, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)]\n",
      "File \u001b[1;32mC:\\Users\\ThisPC\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:519\u001b[0m, in \u001b[0;36m_BaseDiscreteNB._check_X\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    517\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_X\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    518\u001b[0m     \u001b[38;5;124;03m\"\"\"Validate X, used only in predict* methods.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 519\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Users\\ThisPC\\anaconda3\\lib\\site-packages\\sklearn\\base.py:585\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    582\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 585\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    587\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mC:\\Users\\ThisPC\\anaconda3\\lib\\site-packages\\sklearn\\base.py:400\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 400\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    401\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    402\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    403\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 27337 features, but MultinomialNB is expecting 36622 features as input."
     ]
    }
   ],
   "source": [
    "nb_classifier.fit(tf_idf_data_train, target)\n",
    "nb_train_preds = nb_classifier.predict(tf_idf_data_train)\n",
    "nb_test_preds = nb_classifier.predict(tf_idf_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 27337 features, but RandomForestClassifier is expecting 36622 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [35]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m rf_classifier\u001b[38;5;241m.\u001b[39mfit(tf_idf_data_train, target)\n\u001b[0;32m      2\u001b[0m rf_train_preds \u001b[38;5;241m=\u001b[39m rf_classifier\u001b[38;5;241m.\u001b[39mpredict(tf_idf_data_train)\n\u001b[1;32m----> 3\u001b[0m rf_test_preds \u001b[38;5;241m=\u001b[39m \u001b[43mrf_classifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf_idf_data_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Users\\ThisPC\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:808\u001b[0m, in \u001b[0;36mForestClassifier.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    788\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    789\u001b[0m \u001b[38;5;124;03m    Predict class for X.\u001b[39;00m\n\u001b[0;32m    790\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    806\u001b[0m \u001b[38;5;124;03m        The predicted classes.\u001b[39;00m\n\u001b[0;32m    807\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 808\u001b[0m     proba \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    810\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    811\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;241m.\u001b[39mtake(np\u001b[38;5;241m.\u001b[39margmax(proba, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mC:\\Users\\ThisPC\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:850\u001b[0m, in \u001b[0;36mForestClassifier.predict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    848\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    849\u001b[0m \u001b[38;5;66;03m# Check data\u001b[39;00m\n\u001b[1;32m--> 850\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_X_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    852\u001b[0m \u001b[38;5;66;03m# Assign chunk of trees to jobs\u001b[39;00m\n\u001b[0;32m    853\u001b[0m n_jobs, _, _ \u001b[38;5;241m=\u001b[39m _partition_estimators(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_estimators, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n",
      "File \u001b[1;32mC:\\Users\\ThisPC\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:579\u001b[0m, in \u001b[0;36mBaseForest._validate_X_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    576\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    577\u001b[0m \u001b[38;5;124;03mValidate X whenever one tries to predict, apply, predict_proba.\"\"\"\u001b[39;00m\n\u001b[0;32m    578\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 579\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(X) \u001b[38;5;129;01mand\u001b[39;00m (X\u001b[38;5;241m.\u001b[39mindices\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc \u001b[38;5;129;01mor\u001b[39;00m X\u001b[38;5;241m.\u001b[39mindptr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc):\n\u001b[0;32m    581\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo support for np.int64 index based sparse matrices\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\Users\\ThisPC\\anaconda3\\lib\\site-packages\\sklearn\\base.py:585\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    582\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 585\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    587\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mC:\\Users\\ThisPC\\anaconda3\\lib\\site-packages\\sklearn\\base.py:400\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 400\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    401\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    402\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    403\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 27337 features, but RandomForestClassifier is expecting 36622 features as input."
     ]
    }
   ],
   "source": [
    "rf_classifier.fit(tf_idf_data_train, target)\n",
    "rf_train_preds = rf_classifier.predict(tf_idf_data_train)\n",
    "rf_test_preds = rf_classifier.predict(tf_idf_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported format string passed to NoneType.__format__",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [30]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m rf_test_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMultinomial Naive Bayes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTraining Accuracy: \u001b[39;49m\u001b[38;5;132;43;01m{:.4}\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;124;43m Testing Accuracy: \u001b[39;49m\u001b[38;5;132;43;01m{:.4}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnb_train_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnb_test_score\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m70\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported format string passed to NoneType.__format__"
     ]
    }
   ],
   "source": [
    "nb_train_score = accuracy_score(target, nb_train_preds)\n",
    "nb_test_score = accuracy_score(newsgroups_test.target, nb_test_preds)\n",
    "rf_train_score = accuracy_score(target, rf_train_preds)\n",
    "rf_test_score = accuracy_score(newsgroups_test.target, rf_test_preds)\n",
    "\n",
    "print(\"Multinomial Naive Bayes\")\n",
    "print(\"Training Accuracy: {:.4} \\t\\t Testing Accuracy: {:.4}\".format(nb_train_score, nb_test_score))\n",
    "print(\"\")\n",
    "print('-'*70)\n",
    "print(\"\")\n",
    "print('Random Forest')\n",
    "print(\"Training Accuracy: {:.4} \\t\\t Testing Accuracy: {:.4}\".format(rf_train_score, rf_test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting Results\n",
    "\n",
    "**_Question:_** Interpret the results seen above. How well did the models do? How do they compare to random guessing? How would you describe the quality of the model fit?\n",
    "\n",
    "Write your answer below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your answer here\n",
    "The models did well. Since there are five classes, the naive accuracy rate (random guessing) would be 20%. \n",
    "With scores of 78 and 81 percent, the models did much better than random guessing. \n",
    "There is some evidence of overfitting, as the scores on the training set are much higher than those of the test set. \n",
    "This suggests that the models' fits could be improved with some tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "In this lab, we used our NLP skills to clean, preprocess, explore, and fit models to text data for classification. This wasn't easy -- great job!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
